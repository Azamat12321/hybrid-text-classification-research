{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "from torch import save\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Проверка наличия GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Чтение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120000 entries, 0 to 119999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   Class Index  120000 non-null  int64 \n",
      " 1   Title        120000 non-null  object\n",
      " 2   Description  120000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.7+ MB\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7600 entries, 0 to 7599\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Class Index  7600 non-null   int64 \n",
      " 1   Title        7600 non-null   object\n",
      " 2   Description  7600 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 178.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\", encoding = 'latin')\n",
    "test = pd.read_csv(\"test.csv\", encoding = \"latin\")\n",
    "\n",
    "colms = [\"Class Index\", \"Title\", \"Description\"]\n",
    "train.columns = colms\n",
    "test.columns = colms\n",
    "\n",
    "train.info()\n",
    "print(\"\\n\")\n",
    "test.info()\n",
    "\n",
    "\n",
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class Index                                              Title  \\\n",
       "0            3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1            3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "\n",
       "                                         Description  \n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1  Reuters - Private investment firm Carlyle Grou...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train\n",
    "test_df = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class Index                                              Title  \\\n",
       "0            3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1            3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2            3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3            3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4            3  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...   \n",
       "1  Reuters - Private investment firm Carlyle Grou...   \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...   \n",
       "3  Reuters - Authorities have halted oil export\\f...   \n",
       "4  AFP - Tearaway world oil prices, toppling reco...   \n",
       "\n",
       "                                                Text  \n",
       "0  Wall St. Bears Claw Back Into the Black (Reute...  \n",
       "1  Carlyle Looks Toward Commercial Aerospace (Reu...  \n",
       "2  Oil and Economy Cloud Stocks' Outlook (Reuters...  \n",
       "3  Iraq Halts Oil Exports from Main Southern Pipe...  \n",
       "4  Oil prices soar to all-time record, posing new...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combine_text(row):\n",
    "    return f\"{row['Title']} - {row['Description']}\"\n",
    "\n",
    "train_df['Text'] = train_df.apply(combine_text, axis=1)\n",
    "test_df['Text'] = test_df.apply(combine_text, axis=1)\n",
    "\n",
    "\n",
    "# train_df['Text'] = train_df[\"Description\"]\n",
    "# test_df['Text'] = test_df[\"Description\"]\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Шаг 1: Очистка текста\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text =  re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "    text =  re.sub(r'@[^\\s]+', ' ', text)\n",
    "\n",
    "    text =  re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))',' ', text)\n",
    "\n",
    "    text =  re.sub(r'[0-9]+', '', text)\n",
    "\n",
    "    # Удаление HTML-тегов\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Удаление ссылок\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    text = \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "    \n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    # Удаление HTML-тегов\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Удаление ссылок\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Пример очистки текста\n",
    "train_df['Text'] = train_df['Text'].apply(clean_text)\n",
    "test_df['Text'] = test_df['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "  def __init__(self,df):\n",
    "    self.n_samples = len(df)\n",
    "    self.dataframe = df\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    row = self.dataframe.iloc[index]\n",
    "    return row['Class Index'], row['Text']\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we convert the dataframe for the training and testing into datasets\n",
    "train_dataset = NewsDataset(train_df)\n",
    "test_dataset = NewsDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext==0.5.0\n",
      "  Downloading torchtext-0.5.0-py3-none-any.whl (73 kB)\n",
      "     -------------------------------------- 73.2/73.2 kB 669.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: sentencepiece in s:\\python_vs\\datascience\\venv\\lib\\site-packages (from torchtext==0.5.0) (0.2.0)\n",
      "Requirement already satisfied: requests in s:\\python_vs\\datascience\\venv\\lib\\site-packages (from torchtext==0.5.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm in s:\\python_vs\\datascience\\venv\\lib\\site-packages (from torchtext==0.5.0) (4.67.0)\n",
      "Requirement already satisfied: numpy in s:\\python_vs\\datascience\\venv\\lib\\site-packages (from torchtext==0.5.0) (1.26.4)\n",
      "Requirement already satisfied: six in s:\\python_vs\\datascience\\venv\\lib\\site-packages (from torchtext==0.5.0) (1.16.0)\n",
      "Requirement already satisfied: torch in s:\\python_vs\\datascience\\venv\\lib\\site-packages (from torchtext==0.5.0) (2.5.1+cu124)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in s:\\python_vs\\datascience\\venv\\lib\\site-packages (from requests->torchtext==0.5.0) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in s:\\python_vs\\datascience\\venv\\lib\\site-packages (from requests->torchtext==0.5.0) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in s:\\python_vs\\datascience\\venv\\lib\\site-packages (from requests->torchtext==0.5.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in s:\\python_vs\\datascience\\venv\\lib\\site-packages (from requests->torchtext==0.5.0) (2.2.3)\n",
      "Requirement already satisfied: jinja2 in s:\\python_vs\\datascience\\venv\\lib\\site-packages (from torch->torchtext==0.5.0) (3.1.4)\n",
      "Requirement already satisfied: networkx in s:\\python_vs\\datascience\\venv\\lib\\site-packages (from torch->torchtext==0.5.0) (3.4.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in s:\\python_vs\\datascience\\venv\\lib\\site-packages (from torch->torchtext==0.5.0) (4.12.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in s:\\python_vs\\datascience\\venv\\lib\\site-packages (from torch->torchtext==0.5.0) (1.13.1)\n",
      "Requirement already satisfied: filelock in s:\\python_vs\\datascience\\venv\\lib\\site-packages (from torch->torchtext==0.5.0) (3.16.1)\n",
      "Requirement already satisfied: fsspec in s:\\python_vs\\datascience\\venv\\lib\\site-packages (from torch->torchtext==0.5.0) (2024.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in s:\\python_vs\\datascience\\venv\\lib\\site-packages (from sympy==1.13.1->torch->torchtext==0.5.0) (1.3.0)\n",
      "Requirement already satisfied: colorama in s:\\python_vs\\datascience\\venv\\lib\\site-packages (from tqdm->torchtext==0.5.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in s:\\python_vs\\datascience\\venv\\lib\\site-packages (from jinja2->torch->torchtext==0.5.0) (3.0.2)\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchtext==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import collections\n",
    "import torchtext\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    counter.update(torchtext.data.utils.ngrams_iterator(tokenizer(line), ngrams=1))\n",
    "vocab = torchtext.vocab.Vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size if 95131\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size if {vocab_size}\")\n",
    "\n",
    "def encode(x):\n",
    "    return [vocab.stoi[s] for s in tokenizer(x)]\n",
    "\n",
    "def decode(x):\n",
    "    return [vocab.itos[i] for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padify(b):\n",
    "    v = [encode(x[1]) for x in b]\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0]-1 for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class RNNClassifierWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_class):\n",
    "        super(RNNClassifierWithAttention, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Эмбеддинг\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)  # LSTM для обработки последовательностей\n",
    "        self.attention = nn.Linear(hidden_dim, 1)  # Слой внимания\n",
    "        self.fc = nn.Linear(hidden_dim, num_class)  # Полносвязный слой для классификации\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        x = self.embedding(x)  # Эмбеддинг\n",
    "        rnn_out, (hidden, cell) = self.rnn(x)  # Проходим через LSTM\n",
    "        # Применяем внимание\n",
    "        attention_scores = torch.tanh(self.attention(rnn_out))        # [batch_size, seq_len, 1]  \n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)    # [batch_size, seq_len, 1]\n",
    "        \n",
    "        # Получаем контекстный вектор, взвешивая выходы RNN\n",
    "        context_vector = torch.sum(attention_weights * rnn_out, dim=1)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Применяем полносвязный слой для классификации\n",
    "        output = self.fc(context_vector)  # [batch_size, num_class]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "class CNNRNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, cnn_out_channels, kernel_size, hidden_dim, num_classes):\n",
    "        super(CNNRNNClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Слой эмбеддинга\n",
    "        self.conv1d = nn.Conv1d(in_channels=embedding_dim, \n",
    "                               out_channels=cnn_out_channels, \n",
    "                               kernel_size=kernel_size, \n",
    "                               padding=kernel_size // 2)  # 1D свертка\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=cnn_out_channels, hidden_size=hidden_dim, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)  # Полносвязный слой\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Преобразование в эмбеддинги [batch_size, seq_len, embedding_dim]\n",
    "        x = x.permute(0, 2, 1)  # Перестановка для Conv1d [batch_size, embedding_dim, seq_len]\n",
    "        \n",
    "        x = F.relu(self.conv1d(x))  # Применение свертки и активации ReLU\n",
    "        x = x.permute(0, 2, 1)  # Обратно в форму [batch_size, seq_len, cnn_out_channels]\n",
    "        \n",
    "        rnn_out, (hidden, cell) = self.rnn(x)  # Передача в LSTM\n",
    "        output = self.fc(hidden[-1])  # Используем последний скрытый слой\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, collate_fn=padify, shuffle=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 12573317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 61\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Запуск обучения\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save\n\u001b[0;32m     64\u001b[0m save(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCNN+RNN.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 32\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[0;32m     28\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     30\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m labels, inputs \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[0;32m     33\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     35\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Обнуляем градиенты\u001b[39;00m\n",
      "File \u001b[1;32ms:\\PYTHON_VS\\DataScience\\venv\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32ms:\\PYTHON_VS\\DataScience\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32ms:\\PYTHON_VS\\DataScience\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32ms:\\PYTHON_VS\\DataScience\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m, in \u001b[0;36mpadify\u001b[1;34m(b)\u001b[0m\n\u001b[0;32m      2\u001b[0m v \u001b[38;5;241m=\u001b[39m [encode(x[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m b]\n\u001b[0;32m      3\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlen\u001b[39m,v))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ( \u001b[38;5;66;03m# tuple of two tensors - labels and features\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     torch\u001b[38;5;241m.\u001b[39mLongTensor([t[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m b]),\n\u001b[1;32m----> 6\u001b[0m     torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(torch\u001b[38;5;241m.\u001b[39mtensor(t),(\u001b[38;5;241m0\u001b[39m,l\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(t)),mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m,value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m v])\n\u001b[0;32m      7\u001b[0m )\n",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m v \u001b[38;5;241m=\u001b[39m [encode(x[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m b]\n\u001b[0;32m      3\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlen\u001b[39m,v))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ( \u001b[38;5;66;03m# tuple of two tensors - labels and features\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     torch\u001b[38;5;241m.\u001b[39mLongTensor([t[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m b]),\n\u001b[1;32m----> 6\u001b[0m     torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconstant\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m v])\n\u001b[0;32m      7\u001b[0m )\n",
      "File \u001b[1;32ms:\\PYTHON_VS\\DataScience\\venv\\lib\\site-packages\\torch\\nn\\functional.py:5096\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(input, pad, mode, value)\u001b[0m\n\u001b[0;32m   5089\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplicate\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   5090\u001b[0m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put.\u001b[39;00m\n\u001b[0;32m   5091\u001b[0m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[0;32m   5092\u001b[0m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[0;32m   5093\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\n\u001b[0;32m   5094\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch._decomp.decompositions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5095\u001b[0m             )\u001b[38;5;241m.\u001b[39m_replication_pad(\u001b[38;5;28minput\u001b[39m, pad)\n\u001b[1;32m-> 5096\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model = RNNClassifier(vocab_size, 64, 32, 4).to(device)\n",
    "# model = RNNWithAttention(vocab_size, 64, 32, 4).to(device)\n",
    "model = RNNClassifierWithAttention(vocab_size, embedding_dim=128, hidden_dim=256, num_class=4).to(device)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of parameters: {trainable_params}\")\n",
    "# model = CNNRNNClassifier(vocab_size, embedding_dim=300, cnn_out_channels=128, kernel_size=5, hidden_dim=256, num_classes=4).to(device)\n",
    "\n",
    "\n",
    "lr = 0.001\n",
    "report_freq=200\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 2\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device):\n",
    "    model.to(device)  # Переносим модель на устройство (CPU/GPU)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Устанавливаем режим обучения\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "        \n",
    "        for labels, inputs in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()  # Обнуляем градиенты\n",
    "            \n",
    "            outputs = model(inputs)  # Прямой проход\n",
    "            loss = criterion(outputs, labels)  # Вычисление функции потерь\n",
    "            \n",
    "            loss.backward()  # Обратное распространение\n",
    "            optimizer.step()  # Обновление весов\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Расчет accuracy\n",
    "            _, predicted = torch.max(outputs, 1)  # Получаем предсказанные классы\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            acc = correct / total  # Текущая точность\n",
    "            progress_bar.set_postfix(loss=loss.item(), accuracy=acc * 100)\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        final_acc = correct / total * 100\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {final_acc:.2f}%\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "# Запуск обучения\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "from torch import save\n",
    "save(model.state_dict(), \"CNN+RNN.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#        World       0.94      0.91      0.92      1900\n",
    "#       Sports       0.95      0.98      0.96      1900\n",
    "#     Business       0.90      0.85      0.88      1900\n",
    "#     Sci/Tech       0.86      0.91      0.89      1900\n",
    "\n",
    "#     accuracy                           0.91      7600\n",
    "#    macro avg       0.91      0.91      0.91      7600\n",
    "# weighted avg       0.91      0.91      0.91      7600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#        World       0.93      0.90      0.92      1900\n",
    "#       Sports       0.96      0.97      0.97      1900\n",
    "#     Business       0.85      0.91      0.88      1900\n",
    "#     Sci/Tech       0.90      0.86      0.88      1900\n",
    "\n",
    "#     accuracy                           0.91      7600\n",
    "#    macro avg       0.91      0.91      0.91      7600\n",
    "# weighted avg       0.91      0.91      0.91      7600\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Оценка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_16980\\1130644261.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"models/final/CNN+RNN_final.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World     0.8974    0.9158    0.9065      1900\n",
      "      Sports     0.9461    0.9800    0.9628      1900\n",
      "    Business     0.8960    0.8568    0.8760      1900\n",
      "    Sci/Tech     0.8918    0.8805    0.8861      1900\n",
      "\n",
      "    accuracy                         0.9083      7600\n",
      "   macro avg     0.9078    0.9083    0.9078      7600\n",
      "weighted avg     0.9078    0.9083    0.9078      7600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_model_with_accuracy(model, test_loader, vocab, classes, device):\n",
    "    model.eval()  # Переводим модель в режим оценки (без градиентов)\n",
    "    \n",
    "    correct = 0  # Количество правильных предсказаний\n",
    "    total = 0  # Общее количество примеров\n",
    "    with torch.no_grad():  # Не вычисляем градиенты во время тестирования\n",
    "        for batch_idx, (target, data) in enumerate(test_loader):\n",
    "            \n",
    "            # Перенос данных и меток на устройство\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Получение предсказаний\n",
    "            pred = model(data)\n",
    "            \n",
    "            # Вычисляем количество правильных предсказаний\n",
    "            _, predicted = torch.max(pred, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "        # Вычисление точности\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Accuracy of the model on the test data: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, collate_fn=padify, shuffle=True)\n",
    "\n",
    "# Загрузка модели (определите вашу архитектуру перед загрузкой)\n",
    "# model = RNNClassifierWithAttention(vocab_size, 64, 32, 4).to(device)  \n",
    "# model = RNNClassifier(vocab_size, 64, 32, 4).to(device)  # 90_75\n",
    "model = CNNRNNClassifier(vocab_size, embedding_dim=300, cnn_out_channels=128, kernel_size=5, hidden_dim=256, num_classes=4).to(device)\n",
    "model.load_state_dict(torch.load(\"models/final/CNN+RNN_final.pth\"))\n",
    "model.eval()  # Устанавливаем модель в режим оценки\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Получаем прогнозы и истинные метки\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        labels, text = batch\n",
    "        text, labels = text.to(device), labels.to(device)\n",
    "\n",
    "        # Получаем выходы модели\n",
    "        outputs = model(text)\n",
    "        predicted_classes = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        all_preds.extend(predicted_classes.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Создаем отчет классификации\n",
    "report = classification_report(all_labels, all_preds, target_names=classes, digits=4)\n",
    "print(report)\n",
    "\n",
    "# test_model_with_accuracy(model, test_loader, vocab, classes, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(model.state_dict(), \"RNN + Attetion_final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TITLE + DESC без очистки\n",
    "## RNN:\n",
    "### train = 87.49%\n",
    "### test = 90.12%\n",
    "\n",
    "## RNN + Attetion V1:\n",
    "### train = 88.87%\n",
    "### test = 90.74%\n",
    "\n",
    "## RNN + Attetion V2:\n",
    "### train = 89.07%\n",
    "### test = 90.53%\n",
    "\n",
    "# TITLE + DESC без очистки\n",
    "## RNN:\n",
    "### train = 86.85%\n",
    "### test = 90.75%\n",
    "\n",
    "## RNN + Attetion V1:\n",
    "### train = 88.96%\n",
    "### test = 91.31%\n",
    "\n",
    "## RNN + Attetion V2:\n",
    "### train = 89.07%\n",
    "### test = 91.57%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
