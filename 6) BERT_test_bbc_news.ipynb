{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\PYTHON_VS\\DataScience\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "from torch import save\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "# Проверка наличия GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Чтение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1490 entries, 0 to 1489\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   ArticleId  1490 non-null   int64 \n",
      " 1   Text       1490 non-null   object\n",
      " 2   Category   1490 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 35.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "base_path = \"../NLP_BBCNEWS/\"\n",
    "\n",
    "df_solution = pd.read_csv(f\"{base_path}BBC News Sample Solution.csv\")\n",
    "df_test =  pd.read_csv(f\"{base_path}BBC News Test.csv\")\n",
    "df_train = pd.read_csv(f\"{base_path}BBC News Train.csv\")\n",
    "print(df_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_id\n",
       "2    346\n",
       "4    336\n",
       "1    274\n",
       "3    261\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformation_dict = {\n",
    "    'business': 4,        # 'business' - 2 = business\n",
    "    'tech': 3,            # 'tech' - 3 = sci/tech\n",
    "    'politics': 1,        # 'politics' - 0 = world\n",
    "    'sport': 2,           # 'sport' - 1 = sport\n",
    "}\n",
    "\n",
    "df_train['category_id'] = df_train.Category.map(transformation_dict)\n",
    "df_train = df_train.drop(columns=[\"ArticleId\"])\n",
    "df_train = df_train.dropna().reset_index().drop(columns=[\"index\"])\n",
    "\n",
    "df_train[\"category_id\"] = df_train[\"category_id\"].astype(np.int8)\n",
    "df_train[\"category_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    # Удаление HTML-тегов\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Удаление ссылок\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Пример очистки текста\n",
    "df_train['Text'] = df_train['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): DistilBertSdpaAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Инициализация BERT\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "\n",
    "distil_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "distil_bert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
    "\n",
    "bert_model.eval()           # Устанавливаем режим оценки (чтобы заморозить веса)\n",
    "distil_bert_model.eval()    # Устанавливаем режим оценки (чтобы заморозить веса)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1018</td>\n",
       "      <td>qpr keeper day heads for preston queens park r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1319</td>\n",
       "      <td>software watching while you work software that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1138</td>\n",
       "      <td>d arcy injury adds to ireland woe gordon d arc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>459</td>\n",
       "      <td>india s reliance family feud heats up the ongo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1020</td>\n",
       "      <td>boro suffer morrison injury blow middlesbrough...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>1923</td>\n",
       "      <td>eu to probe alitalia  state aid  the european ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>373</td>\n",
       "      <td>u2 to play at grammy awards show irish rock ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>1704</td>\n",
       "      <td>sport betting rules in spotlight a group of mp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>206</td>\n",
       "      <td>alfa romeos  to get gm engines  fiat is to sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>471</td>\n",
       "      <td>citizenship event for 18s touted citizenship c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>735 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ArticleId                                               Text\n",
       "0         1018  qpr keeper day heads for preston queens park r...\n",
       "1         1319  software watching while you work software that...\n",
       "2         1138  d arcy injury adds to ireland woe gordon d arc...\n",
       "3          459  india s reliance family feud heats up the ongo...\n",
       "4         1020  boro suffer morrison injury blow middlesbrough...\n",
       "..         ...                                                ...\n",
       "730       1923  eu to probe alitalia  state aid  the european ...\n",
       "731        373  u2 to play at grammy awards show irish rock ba...\n",
       "732       1704  sport betting rules in spotlight a group of mp...\n",
       "733        206  alfa romeos  to get gm engines  fiat is to sto...\n",
       "734        471  citizenship event for 18s touted citizenship c...\n",
       "\n",
       "[735 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для извлечения признаков\n",
    "def extract_features_in_batches(texts, tokenizer, bert_model, batch_size=32, max_length=128, info=\"data\"):\n",
    "    features = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=f\"Extracting BERT features for {info}:\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        tokens = tokenizer(batch_texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        input_ids = tokens[\"input_ids\"].to(device)\n",
    "        attention_mask = tokens[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]  # Извлекаем [CLS]\n",
    "            features.append(cls_embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(features)\n",
    "\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[\"Text\"], df_train[\"category_id\"], test_size=0.15, random_state=42)\n",
    "\n",
    "use_model = distil_bert_model\n",
    "use_tokenizer = distil_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Загрузка/Извлечение признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Загрузка признаков\n",
    "    with open(\"features/train_features.pkl\", \"rb\") as f:\n",
    "        X_train_features, y_train = pickle.load(f)\n",
    "    with open(\"features/test_features.pkl\", \"rb\") as f:\n",
    "        X_test_features, y_test = pickle.load(f)\n",
    "\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    # Извлечение признаков\n",
    "    X_train_features = extract_features_in_batches(X_train.tolist(), use_tokenizer, use_model, batch_size=32, max_length=64, info=\"train_data\")\n",
    "    X_test_features = extract_features_in_batches(X_test.tolist(), use_tokenizer, use_model, batch_size=32, max_length=64, info=\"test_data\")\n",
    "\n",
    "    # Сохранение\n",
    "    with open(\"features/train_features.pkl\", \"wb\") as f:\n",
    "        pickle.dump((X_train_features, y_train), f)\n",
    "    with open(\"features/test_features.pkl\", \"wb\") as f:\n",
    "        pickle.dump((X_test_features, y_test), f)\n",
    "\n",
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9105\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.93      0.90      0.91      1900\n",
      "      Sports       0.97      0.98      0.98      1900\n",
      "    Business       0.86      0.87      0.87      1900\n",
      "    Sci/Tech       0.88      0.89      0.88      1900\n",
      "\n",
      "    accuracy                           0.91      7600\n",
      "   macro avg       0.91      0.91      0.91      7600\n",
      "weighted avg       0.91      0.91      0.91      7600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Обучение классификатора (например, логистическая регрессия)\n",
    "clf = LogisticRegression(max_iter=2000)  # Также можно использовать SVM или DecisionTreeClassifier\n",
    "clf.fit(X_train_features, y_train)\n",
    "\n",
    "# Предсказание и оценка\n",
    "y_pred = clf.predict(X_test_features)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "report = classification_report(y_test, y_pred, target_names=classes)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2. LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.653342 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "Accuracy: 0.8905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.91      0.88      0.89      1900\n",
      "      Sports       0.96      0.98      0.97      1900\n",
      "    Business       0.84      0.84      0.84      1900\n",
      "    Sci/Tech       0.86      0.86      0.86      1900\n",
      "\n",
      "    accuracy                           0.89      7600\n",
      "   macro avg       0.89      0.89      0.89      7600\n",
      "weighted avg       0.89      0.89      0.89      7600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "model = lgb.LGBMClassifier(n_estimators=50, random_state=42)\n",
    "model.fit(X_train_features, y_train)\n",
    "y_pred = model.predict(X_test_features)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "report = classification_report(y_test, y_pred, target_names=classes)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
